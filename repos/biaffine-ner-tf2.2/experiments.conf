# Word embeddings.
ft_en_300d {
  path = ./crawl-300d-2M.vec 
  size = 300
}
#ft_de_300d {
#  path = ../fasttext/cc.de.300.vec.filtered
#  size = 300
#}
#ft_nl_300d {
#  path = ../fasttext/cc.nl.300.vec.filtered
#  size = 300
#}
#ft_es_300d {
#  path = ../fasttext/cc.es.300.vec.filtered
#  size = 300
#}
ft_ru_300d{
  path = ../pretrained/cc.ru.300.vec
  size = 300
}
#w2v_cop_50d{
#  path = vec/coptic_50d.vec
#  size = 50
#}

base  {
  ffnn_size = 150
  ffnn_depth = 2
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = ""
  context_embeddings = ${ft_en_300d}
  contextualization_size = 200
  contextualization_layers = 3
  lm_size = 1024
  lm_layers = 4
  lm_path = ""

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = ""
  eval_path = ""
  lm_path = ""
  test_path = ""
  ner_types = []
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
  max_step = 40000
}


experiment1 = ${base}{
  train_path = data/21.01.2021/train.jsonlines
  eval_path = data/21.01.2021/dev.jsonlines
  test_path = data/21.01.2021/dev.jsonlines
  context_embeddings = ${ft_ru_300d}
  lm_path = none
  report_frequency = 250
  ner_types = [
    "AGE", "AWARD", "CHARGE", "CITY", "COUNTRY", "CRIME", "DATE", "DISEASE", "DISTRICT", "EVENT", "FACILITY", "FAMILY", "IDEOLOGY", "LANGUAGE", "LAW", "LOCATION", "MONEY", "NATIONALITY", 
    "NUMBER", "ORDINAL", "ORGANIZATION", "PENALTY", "PERCENT", "PERSON", "PRODUCT", "PROFESSION", "RELIGION", "STATE_OR_PROVINCE", "TIME", "WORK_OF_ART"
  ]
  char_vocab_path = data/21.01.2021/char_vocab.txt
  max_step = 80000
}

experiment2 = ${base}{
  train_path = data/21.01.2021/train.jsonlines
  eval_path = data/21.01.2021/dev.jsonlines
  test_path = data/21.01.2021/dev.jsonlines
  context_embeddings = ${ft_ru_300d}
  lm_size = 768
  lm_path = data/21.01.2021/bert_features.hdf5
  report_frequency = 1000
  ner_types = [
    "AGE", "AWARD", "CHARGE", "CITY", "COUNTRY", "CRIME", "DATE", "DISEASE", "DISTRICT", "EVENT", "FACILITY", "FAMILY", "IDEOLOGY", "LANGUAGE", "LAW", "LOCATION", "MONEY", "NATIONALITY", 
    "NUMBER", "ORDINAL", "ORGANIZATION", "PENALTY", "PERCENT", "PERSON", "PRODUCT", "PROFESSION", "RELIGION", "STATE_OR_PROVINCE", "TIME", "WORK_OF_ART"
  ]
  char_vocab_path = data/21.01.2021/char_vocab.txt
  max_step = 80000
}


experiment3 = ${base}{
  train_path = data/25.01.2021/train.jsonlines
  eval_path = data/25.01.2021/dev.jsonlines
  test_path = data/25.01.2021/dev.jsonlines #data/25.01.2021/medical-test.jsonlines #
  context_embeddings = ${ft_ru_300d}
  lm_path = none
  report_frequency = 250
  ner_types = [
    "AGE", "AWARD", "CHARGE", "CITY", "COUNTRY", "CRIME", "DATE", "DISEASE", "DISTRICT", "EVENT", "FACILITY", "FAMILY", "IDEOLOGY", "LANGUAGE", "LAW", "LOCATION", "MONEY", "NATIONALITY", 
    "NUMBER", "ORDINAL", "ORGANIZATION", "PENALTY", "PERCENT", "PERSON", "PRODUCT", "PROFESSION", "RELIGION", "STATE_OR_PROVINCE", "TIME", "WORK_OF_ART"
  ]
  char_vocab_path = data/25.01.2021/char_vocab.txt
  max_step = 80000
}

experiment4 = ${base}{
  train_path = data/25.01.2021/train.jsonlines
  eval_path = data/25.01.2021/dev.jsonlines
  test_path = data/25.01.2021/test.jsonlines #data/25.01.2021/medical-test.jsonlines #
  context_embeddings = ${ft_ru_300d}
  lm_size = 768
  lm_path = data/25.01.2021/rubert_features.hdf5
  report_frequency = 250
  ner_types = [
    "AGE", "AWARD", "CHARGE", "CITY", "COUNTRY", "CRIME", "DATE", "DISEASE", "DISTRICT", "EVENT", "FACILITY", "FAMILY", "IDEOLOGY", "LANGUAGE", "LAW", "LOCATION", "MONEY", "NATIONALITY", 
    "NUMBER", "ORDINAL", "ORGANIZATION", "PENALTY", "PERCENT", "PERSON", "PRODUCT", "PROFESSION", "RELIGION", "STATE_OR_PROVINCE", "TIME", "WORK_OF_ART"
  ]
  char_vocab_path = data/25.01.2021/char_vocab.txt
  max_step = 80000
}

experiment5 = ${base}{
  train_path = data/10.04.2021/train.jsonlines
  eval_path = data/10.04.2021/dev.jsonlines
  test_path = data/10.04.2021/test.jsonlines #data/10.04.2021/medical-test.jsonlines #
  context_embeddings = ${ft_ru_300d}
  lm_path = none
  report_frequency = 250
  ner_types = [
    "AGE", "AWARD", "CHARGE", "CITY", "COUNTRY", "CRIME", "DATE", "DISEASE", "DISTRICT", "EVENT", "FACILITY", "FAMILY", "IDEOLOGY", "LANGUAGE", "LAW", "LOCATION", "MONEY", "NATIONALITY", 
    "NUMBER", "ORDINAL", "ORGANIZATION", "PENALTY", "PERCENT", "PERSON", "PRODUCT", "PROFESSION", "RELIGION", "STATE_OR_PROVINCE", "TIME", "WORK_OF_ART"
  ]
  char_vocab_path = data/10.04.2021/char_vocab.txt
  max_step = 80000
}

experiment6 = ${base}{
  train_path = data/10.04.2021/train.jsonlines
  eval_path = data/10.04.2021/dev.jsonlines
  test_path = data/10.04.2021/test.jsonlines #data/10.04.2021/medical-test.jsonlines #
  context_embeddings = ${ft_ru_300d}
  lm_size = 768
  lm_path = data/10.04.2021/rubert_features.hdf5
  report_frequency = 250
  ner_types = [
    "AGE", "AWARD", "CHARGE", "CITY", "COUNTRY", "CRIME", "DATE", "DISEASE", "DISTRICT", "EVENT", "FACILITY", "FAMILY", "IDEOLOGY", "LANGUAGE", "LAW", "LOCATION", "MONEY", "NATIONALITY", 
    "NUMBER", "ORDINAL", "ORGANIZATION", "PENALTY", "PERCENT", "PERSON", "PRODUCT", "PROFESSION", "RELIGION", "STATE_OR_PROVINCE", "TIME", "WORK_OF_ART"
  ]
  char_vocab_path = data/25.01.2021/char_vocab.txt
  max_step = 80000
}

# Main configuration.
#eng_genia = ${base}{
#  train_path = train_dev.genia.jsonlines
#  test_path = test.genia.jsonlines
#  lm_path = bert-model/bert_genia_features.hdf5
#  ner_types = ["DNA","RNA","protein","cell_line","cell_type"]
#  char_vocab_path = "char_vocab.eng.genia.txt"
#  max_step = 80000
#}

toxic_spans = ${base}{
  train_path = ./bert-ner-data/train.jsonlines
  eval_path = ./bert-ner-data/dev.jsonlines
  lm_path = bert-model/bert_ace2004_features.hdf5
  test_path = ./bert-ner-data/test.jsonlines
  ner_types = ["O", "TOX"]
  char_vocab_path = "char_vocab.eng.ace2004.txt"
  max_step = 40000
}


#eng_ace04 = ${base}{
#  train_path = train.ACE2004.jsonlines
#  eval_path = dev.ACE2004.jsonlines
#  lm_path = bert-model/bert_ace2004_features.hdf5
#  test_path = test.ACE2004.jsonlines
#  ner_types = ["LOC","WEA","GPE","PER","FAC","ORG","VEH"]
#  char_vocab_path = "char_vocab.eng.ace2004.txt"
#  max_step = 40000
#}

#eng_ace05 = ${base}{
#  train_path = train.ACE2005.jsonlines
#  eval_path = dev.ACE2005.jsonlines
#  lm_path = bert-model/bert_ace2005_features.hdf5
#  test_path = test.ACE2005.jsonlines
#  ner_types = ["LOC","WEA","GPE","PER","FAC","ORG","VEH"]
#  char_vocab_path = "char_vocab.eng.ace2005.txt"
#  max_step = 100000
#}

#eng_conll12 = ${base}{
#  train_path = train.conll12.jsonlines
#  eval_path = dev.conll12.jsonlines
#  lm_path = bert-model/bert_conll2012_features.hdf5  test_path = test.conll12.jsonlines
#  ner_types = ["ORDINAL","LOC","PRODUCT","NORP","WORK_OF_ART","LANGUAGE","GPE","TIME","PERCENT","MONEY","PERSON","CARDINAL","FAC","DATE","ORG","LAW","EVENT","QUANTITY"]
#  char_vocab_path = "char_vocab.eng.conll12.txt"
#  flat_ner = true
#  max_step = 200000
#}

#eng_conll03 = ${base}{
#  train_path = train_dev.conll03.jsonlines
#  lm_path = bert-model/bert_conll03_features.hdf5
#  test_path = test.conll03.jsonlines
#  ner_types = ["ORG","MISC","PER","LOC"]
#  char_vocab_path = "char_vocab.eng.conll03.txt"
#  flat_ner = true
#  max_step = 80000
#}

#deu_conll03 = ${base}{
#  train_path = train.deu.conll03.corrected.jsonlines
#  lm_path = bert-model/bert_deu_conll03_features.hdf5
#  test_path = test.deu.conll03.corrected.jsonlines
#  ner_types = ["ORG","MISC","PER","LOC"]
#  char_vocab_path = "char_vocab.deu.conll03.txt"
#  context_embeddings = ${ft_de_300d}
#  lm_size = 768
#  flat_ner = true
#  max_step = 100000
#}

#deu_conll03_revised = ${deu_conll03}{
#  train_path = train_dev.deu.conll03.revised06.jsonlines
#  test_path = test.deu.conll03.revised06.jsonlines
#}

#esp_conll02 = ${base}{
#  train_path = train_dev.esp.conll02.jsonlines
#  lm_path = bert-model/bert_esp_conll02_features.hdf5
#  test_path = test.esp.conll02.jsonlines
#  ner_types = ["ORG","MISC","PER","LOC"]
#  char_vocab_path = "char_vocab.esp.conll02.txt"
#  context_embeddings = ${ft_es_300d}
#  lm_size = 768
#  flat_ner = true
#}

#ned_conll02 = ${base}{
#  train_path = train_dev.ned.conll02.jsonlines
#  lm_path = bert-model/bert_ned_conll02_features.hdf5
#  test_path = test.ned.conll02.jsonlines
#  ner_types = ["ORG","MISC","PER","LOC"]
#  char_vocab_path = "char_vocab.ned.conll02.txt"
#  context_embeddings = ${ft_nl_300d}
#  lm_size = 768
#  flat_ner = true
#}


# Parameters for experiment using UD Coptic data
#cop = ${base}{
#  train_path = cop_train.jsonlines
#  lm_path = none
#  eval_path = cop_dev.jsonlines
#  test_path = cop_test.jsonlines
#  ner_types = ["thing"]
  # comment out prev line and uncomment next to do 10-class classification instead of binary entity recognition
  #ner_types = ["abstract","animal","object","event","place","organization","time","substance","plant","person"]
#  char_vocab_path = "char_vocab.cop.txt"
#  context_embeddings = ${w2v_cop_50d}
#  lm_size = 1
#  lm_layers = 1
#  flat_ner = false
#  contextualization_size = 40
#  contextualization_layers = 1
#
# eval_frequency = 500
# report_frequency = 200
#  log_root = logs
#  max_step = 8000
  
#  lstm_dropout_rate = 0.1
#  lexical_dropout_rate = 0.1
#  dropout_rate = 0.1
#  learning_rate = 0.001
#  ffnn_size = 50
#  ffnn_depth = 2
#  char_embedding_size = 8
#
#}
